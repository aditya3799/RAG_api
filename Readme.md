# ğŸš€ Boeing 737 Technical Manual RAG API

### Retrieval-Augmented Generation System using Qdrant, Sentence Transformers, BM25, RRF, and Gemini 2.5 Flash

This repository implements a complete **Retrieval-Augmented Generation (RAG)** pipeline designed to answer questions strictly based on the **Boeing 737 Technical Manual**.  
The system uses **PDF â†’ chunk â†’ embed â†’ store â†’ retrieve â†’ generate** stages and returns an answer grounded in the manual along with **relevant manual page numbers**.

---

## ğŸ§  Overview

This RAG system is built using:

- **Python**
- **FastAPI** (API)
- **SentenceTransformers** (embeddings)
- **BAAI/bge-large-en-v1.5** (dense semantic embeddings)
- **BM25** (sparse keyword search)
- **Reciprocal Rank Fusion (RRF)** (hybrid retrieval)
- **Qdrant Cloud** (vector database)
- **Gemini 2.5 Flash** (LLM for grounded answer generation)
- **LangChain** (prompt templating)

You can ask **any Boeing 737 operational or performance-related question**, and the system will:

1. Retrieve the most relevant page chunks  
2. Fuse results using RRF  
3. Generate a concise 1â€“2 sentence answer  
4. Provide page citations (1-based index)

---

## ğŸ“ Repository Structure

```
boeing/
â”œâ”€â”€ main.py                     # FastAPI server & response generation
â”œâ”€â”€ qdrant.py                   # PDF â†’ Chunk â†’ Embed â†’ Upload to Qdrant
â”œâ”€â”€ question.py                 # Hybrid retrieval (Dense + BM25 + RRF)
â”œâ”€â”€ Boeing B737 Manual.pdf      
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # Environment variables (gitignored)
â”œâ”€â”€ .env.example                # Example env file
â””â”€â”€ README.md                   # This file
```

---

## ğŸ—ï¸ Full Architecture Explanation

```
PDF Manual
    â†“
PDF Parsing (pdfplumber)
    â†“
Pagewise cleaning
    â†“
Chunking (300 words, 80 overlap)
    â†“
Embedding (bge-large-en-v1.5)
    â†“
Qdrant Upsert
    â†“
User Query
    â†“
Dense Retrieval (Qdrant cosine similarity)
    â†“
Sparse Retrieval (BM25)
    â†“
Reciprocal Rank Fusion
    â†“
Top-k Chunks
    â†“
Gemini 2.5 Flash (strict grounded prompt)
    â†“
Final Answer + Page Numbers
```

---

## ğŸ§© 1. PDF Ingestion & Chunking (`qdrant.py`)

### Process:

1. Load PDF using **pdfplumber**
2. Extract & clean text on a *per-page* basis
3. Chunk pages into:
   - **300-word chunks**
   - **80-word overlap**
4. Embed using `BAAI/bge-large-en-v1.5`
5. Store in Qdrant with metadata:
   - `page`
   - `chunk_index`
   - `text`
   - `source`

### Chunking example:

```python
def chunk_text(text, chunk_size=300, overlap=80):
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunk = " ".join(words[i:i+chunk_size])
        chunks.append(chunk)
        i += chunk_size - overlap
    return chunks
```

### Key Features:
- âœ… Preserves page-level context
- âœ… Overlapping chunks prevent information loss
- âœ… Batch embedding & upload to Qdrant
- âœ… Handles malformed PDFs gracefully

---

## ğŸ” 2. Hybrid Retrieval (`question.py`)

### Dense Retrieval

Using cosine similarity from Qdrant's vector store.


### Sparse Retrieval (BM25)

Using `rank_bm25` to return keyword-based matches.


### Final return:

- Top-k fused chunks
- Page numbers
- Text snippets

---

## ğŸ¤– 3. Grounded Answer Generation (`main.py`)

### The LLM:
**Gemini 2.5 Flash**

### Prompt rules:

- 1â€“2 polished sentences
- No step-by-step reasoning
- No quoting chunks
- No hallucination
- Cite only page numbers (Pages: x, y)
- If context missing â†’ provide a suggestive fallback


## ğŸ”§ Running the API

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Set environment variables

Create `.env` file:

```env
QDRANT_URL=https://your-cluster.qdrant.cloud:6333
QDRANT_API_KEY=your_qdrant_api_key
Gemini_api_key=AIzaSyDO3RtS0vxjIV_QOfssYX9XON8pPxpipPk
```

### 3ï¸âƒ£ Ingest PDF (one-time setup)

```bash
python qdrant.py
```

This will:
- Parse `Boeing B737 Manual.pdf`
- Chunk and embed all pages
- Upload to Qdrant Cloud

### 4ï¸âƒ£ Start FastAPI server

```bash
python main.py
```

or:

```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

Server will start at:

```
http://localhost:8000
```

---

## ğŸŒ API Endpoint Documentation

### POST `/query`

**Request Body:**

```json
{
  "question": "What is the climb limit weight at 2000 ft and 50Â°C?"
}
```

**Example Response:**

```json
{
  "response": "Based on the dry runway data at 2,000 ft pressure altitude and 50Â°C, the climb limit weight is 52,200 kg (Pages: 4, 5).",
  "pages": [4, 5]
}
```

### Example cURL:

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the recommended flap setting for takeoff?"}'
```

---

## ğŸ§ª Why Hybrid Retrieval?

Pure dense retrieval sometimes fails with:

- Tables
- Performance charts
- Abbreviations
- Non-NLP text
- Numerical value searches

**BM25 helps retrieve:**

- Keywords
- Numbers
- Abbreviations

**RRF fuses both to get:**

- âœ” High recall
- âœ” Stable ranking
- âœ” Lower false positives
- âœ” Minimal irrelevant pages

---

## ğŸ§  Challenges & Solutions

### 1. PDF tables not parsed cleanly

â†’ Solved with overlapping chunking + hybrid retrieval.

### 2. LLM hallucinations

â†’ Solved using strict grounding prompt:
- no quotes
- no invented values
- page-citations-only

### 3. API model mismatch errors

â†’ Use correct model: `gemini-2.5-flash`

### 4. Retrieval instability

â†’ RRF ensures robust ranking.

---

## ğŸš€ Future Improvements

- [ ] Cohere Re-Ranker integration
- [ ] Better table extraction (Camelot/Tabula)
- [ ] Query rewriting (user â†’ manual style query)
- [ ] Chunk merge for large diagrams
- [ ] Response JSON formatting with structured citations
- [ ] Add conversation history/context
- [ ] Deploy to cloud (AWS/GCP/Azure)

---

## ğŸ“„ `.env.example`

```env
QDRANT_URL=https://your-qdrant-cluster.cloud:6333
QDRANT_API_KEY=your_qdrant_api_key_here
Gemini_api_key=your_gemini_api_key_here
```


## ğŸ‘¨â€ğŸ’» Author

**Aditya Kudale**


# RAG_api
